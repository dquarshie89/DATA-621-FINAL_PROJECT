---
title: "DATA 621 Final: Predicting A Mobile Application's Success"
author: "Group 3"
date: "CUNY SPS Spring 2019"
output: pdf_document
---

````{r include = FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning = FALSE)
```  

#Abstract 
In an effort to see what developers can focus on to get more downloads of their mobile application, this project will use various modeling techniques to predict an application’s success. These models will we built using data about apps on Apple’s and Google’s stores. This will provide a methodology to scientifically predict how well an app will do in either store when certain metrics are improved.    

Keywords: Apps, Apple, Google, Poisson, Quasi   

#Introduction  
In 2014 Dong Nguyen revealed that his game Flappy Bird was earning an average of $50,000 a day from in-app ads. At that time, Flappy Bird, an addictive and tough game, had been number 1 on the Apple App Store and Google Play Store for almost a month and was gaining more downloads daily. In fact, by January 2014 the app had 50 million downloads, 68,000 reviews, and held the number 1 spot for most downloaded free game in 53 countries.   

With millions of apps available for download today, many app developers could only dream of the success that Flappy Bird saw. So, did Nguyen know how to make an app so successful, and is there any way for us use app statistics to predict the success of an app? Kaggle has a couple of datasets with mobile application statistics that can possibly help us answer these questions. The datasets contain information around Apple apps on the App Store and Google apps on the Play Store; information centered around ratings, app size, installs, and price. Our idea is to take these datasets to see if we can use several regression methods to predict the successfulness of an app.   

#Methodology  
##Apple  
###Apple Store Data  
For this project, data from the Apple store on 7,197 apps was sourced from Kaggle at this site: https://www.kaggle.com/ramamet4/app-store-apple-data-set-10k-apps. The data content on these apps revolved around the apps’ size, price, ratings, genre, and number of supporting devices. However, there is no data around how many times the app was downloaded. As a workaround, this study used the total number of ratings as a measure of how many times the app was downloaded and therefore how successful it is.   
```{r LoadData}
library('ggplot2')
library('MASS')
library('dplyr')
library('psych')
library('DataExplorer')
library('mice')
library('car')
library('caret')
library('faraway')
library('stargazer')
library('pROC')
library('stringr')
library('corrplot')
library('pscl')

#Load
apps_url <- 'https://raw.githubusercontent.com/dquarshie89/Data-621/master/AppleStore.csv'
apps <- read.csv(apps_url, header=TRUE)
```

##Data Management   
Thankfully none of the data in the Apple Store dataset was missing. Two new fields were added to the dataset, one to distinguish if the app was a game or not and another to see if the total ratings count for the app was over or under the median of the dataset. For the content rating field the ‘+’ symbol was removed and the field was transformed into an integer. Here’s a summary of the final dataset.  

```{r DataManage}
#Make Is_Game and Target
apps$isgame <- ifelse(apps$prime_genre == 'Games', 1, 0)
apps$over_med <- ifelse(apps$rating_count_tot >= median(apps$rating_count_tot), 1, 0)

#Select Final Fields
apps_final <- apps %>% 
  select(size_bytes, price, rating_count_tot, rating_count_ver,	user_rating, user_rating_ver,cont_rating,
         sup_devices.num,	ipadSc_urls.num,	lang.num, isgame, over_med )

#Make Final Edits
apps_final$cont_rating <- unname(sapply(apps_final$cont_rating, str_replace_all, '[+]', ''))
apps_final$cont_rating <- as.numeric(apps_final$cont_rating)

plot_missing(apps_final)
```


###Histogram of Fields  
```{r Histogram}
plot_histogram(apps_final)
```

###Boxplot of Fields  
```{r Boxplot}
plot_boxplot(
  data = apps_final,
  by = "user_rating")+ 
  geom_jitter()
```


###Train – Test Split 
Given that the Apple Store dataset is not conveniently split to do model training, it was split by randomly choosing fifty percent of the data to be in the train set and the remaining data to be in the testing set.  

```{r Split}
#split for Train and Test
set.seed(121)
split <- sample(1:nrow(apps_final), .5*nrow(apps_final))
app_train <- apps_final[-split,]
app_test <- apps_final[split,]

#Models
split2 <- sample(1:nrow(app_train), .8*nrow(app_train))
holdout <- app_train[-split2,]
train <- app_train[split2,]
```

#Results  
##Correlation  
Before beginning the data analysis, the correlation plot between the metrics was analyzed to see if there was anything worth noting.  

```{r Correlation}
corrplot(cor(apps_final),method='color',tl.cex=.5)  
```  


##Ratings Total Summary   
In order to gauge how well the forthcoming models did in predicting the total number of ratings, here’s the summary of ratings from the train dataset.  
```{r TrainSummary}
summary(app_train$rating_count_tot)
```


##Poisson Model  
The first model ran is a Poisson Model.  
```{r Poisson}
#Run with Poisson 
pois <- glm(rating_count_tot ~ . , data=train, family="poisson")
summary(pois)
poispreds <- predict(pois, app_test)
poisddf <- cbind(TARGET_FLAG=exp(poispreds))
summary(poisddf)
```
Running the Poisson model gave the ouput that every field was statistically significant, so they were all kept for the predictions. The results from this model resembles the train's results.  

##Zero Inflated   
Looking at the data set, we can see that there a large number of zeroes in a lot of fields. A zero inflated count regression should help deal with those.  
```{r}
zerodis <- zeroinfl(rating_count_tot ~ . , data=train, dist="poisson")
#summary(zerodis)
zeropreds <- predict(zerodis, app_test)
zerodf <- cbind(TARGET_FLAG=zeropreds)
summary(zerodf)
```  
The zero inflated model also resembles the results of the train's results but underestimates the maximum value. Looking at the distribution of the total ratings count in the overall dataset, we can see that there are only six apps with over one million total ratings, so it can be con be concluded that the zero inflated model's max is acceptable.    

The zero inflated model gave the best results in determining a mobile application's succress. 

##Google Play Store  
This project also investegated what makes an app successful on the Google Play Store. This data was also taken from Kaggle and has 10,839 apps with 14 fields. This data contains similar information as the Apple store data but it also includes how many time the app has been downloaded.   
```{r GoogleData}
google <- read.csv("https://raw.githubusercontent.com/dquarshie89/DATA-621-FINAL_PROJECT/master/googleplaystore.csv",header = TRUE)

#save column names
data_columns <-colnames(google)

column_desc <- c("Application name","Category the app belongs to",
                 "Overall user rating of the app (as when scraped)","Number of user reviews for the app (as when scraped)",
                 "Size of the app (as when scraped)","Number of user downloads/installs for the app (as when scraped)",
                 "Paid or Free","Price of the app (as when scraped)",
                 "Age group the app is targeted at - Children / Mature 21+ / Adult","An app can belong to multiple genres (apart from its main category)","Date when the app was last updated on Play Store (as when scraped)",
                 "Current version of the app available on Play Store (as when scraped)","Min required Android version (as when scraped)")


col_desc <- data.frame(data_columns, column_desc)

#Data information
colnames(col_desc) <- c("Columns","Description")

google_clean <- read.csv("https://raw.githubusercontent.com/dquarshie89/DATA-621-FINAL_PROJECT/master/googleplaystore_clean.csv",header = TRUE)
#Categories of the apps
unique(google_clean$Category)

summary(google_clean)
```  

###Data Management 
There's a lot of missing Rating data and the $ sign has to be removed from the Price field. 
```{r GoogleDataManage}
#select variables
google_clean <- google_clean %>%  select(App, Category, Rating, Reviews, size_adjusted, Installs, Type, Price, Content.Rating, Genres)
google_clean <-google_clean %>% distinct(App,.keep_all = TRUE)
head(google_clean)
plot_missing(google_clean)
```  
```{r CleanPrice}
google_clean <- google_clean %>% filter(!is.na(google_clean$Rating))
google_clean$Price = as.numeric(gsub("\\$", "", google_clean$Price))
```  

##Target Flag  
For the analysis of how succesfull an app is on the Google Play store a target falg was put in place to see if an app was downloaded more times than the average of all the apps.  
```{r Target}
google_clean$above_avg <- ifelse(google_clean$Rating >= mean(google_clean$Rating, na.rm = TRUE), 1, 0)
describe(google_clean)
```  
##Histogram  
```{r}
google_clean %>% filter(Reviews > 20000000)
plot_histogram(google_clean)
```  
##Downloads by Categories  
```{r Cats}
google_clean %>%  group_by(Category) %>%  filter(!is.na(Installs)) %>% summarise(meanInstall = mean(Installs)) %>% 
  ggplot(mapping = aes(x = Category, y = meanInstall)) + geom_col(aes(fill = Category)) + geom_line(group = 1) +  coord_flip() + ggtitle("Average Number of Downloads by Categories") + ylab("Number of Installs") + 
  guides(fill=FALSE)
```  

##Reviews by Categories  
```{r}
google_clean %>%  group_by(Category) %>%  filter(!is.na(Reviews)) %>% summarise(meanReviews = mean(Reviews)) %>% 
  ggplot(mapping = aes(x = Category, y = meanReviews)) + geom_col(aes(fill = Category)) + geom_line(group = 1) +  coord_flip() + ggtitle("Average Reviews by Categories") + ylab("Average Reviews") + 
  guides(fill=FALSE)
```  

##Ratings by Categories  
```{r}
google_clean %>%  group_by(Category) %>%  filter(!is.na(Rating)) %>% summarise(meanRating = mean(Rating)) %>% 
  ggplot(mapping = aes(x = Category, y = meanRating)) + geom_col(aes(fill = Category)) + geom_line(group = 1) +  coord_flip() + ggtitle("Average Rating by Categories") + ylab("Average Rating") + 
  guides(fill=FALSE) 
```  

##Number Above Average by Categories  
```{r}
google_clean %>%  group_by(Category) %>%  filter(!is.na(above_avg)) %>% summarise(total_target= sum(above_avg)) %>% 
  ggplot(mapping = aes(x = Category, y = total_target)) + geom_col(aes(fill = Category)) + geom_line(group = 1) +  coord_flip() + ggtitle("count of Above Average Ratings by Category") + ylab("Average Reviews") + 
  guides(fill=FALSE)
```  

##Log View of Downloads by Categories  
```{r}
google_clean$install_log <- log(google_clean$Installs+1)
google_clean$review_log <- log(google_clean$Reviews+1)
google_clean %>%  group_by(Category) %>%  filter(!is.na(Installs)) %>% summarise(meanInstalls = mean(install_log)) %>% 
  ggplot(mapping = aes(x = Category, y = meanInstalls)) + geom_col(aes(fill = Category)) + geom_line(group = 1) +  coord_flip() + ggtitle("Average Number of Downloads by Categories") + ylab("Number of Installs") + 
  guides(fill=FALSE)
```  

##Train-Test Split   
Just like the Apple data, the Google data did not come in a training and testing datasets. So this was done by splitting up the total datset.  
```{r TestSplit}
set.seed(121)
#Models
split <- sample(1:nrow(google_clean), .8*nrow(google_clean))
experiment <- google_clean[-split,]
train <- google_clean[split,]
```  

##Poisson  
The first model ran was Poisson.  
```{r PosiGoogle}
pois <- glm(above_avg ~ install_log + review_log + Type + Category+ Price + Content.Rating, data=train, family="poisson")
summary(pois)
```

##Poisson Reduced   
We removed some fields from the Poisson model that were not significant.  
```{r PoisRed}
pois_red <- glm(above_avg ~ install_log + review_log + Category+ Price , data=train, family="poisson")
summary(pois_red)
```  
##Quasi 
Next a Quasi model was used.  
```{r Quasi}
quas <- glm(above_avg ~ install_log + review_log + Type + Category+ Price + Content.Rating, data=train , family="quasi")
summary(quas)
```  
##Quasi Reduced   
Quasi with removed fields  
```{r QuasiRed}
quas_red <- glm(above_avg ~ install_log + review_log  + Category+ Price, data=train , family="quasi")
summary(quas_red) 
```  

##Bionmial  
```{r}
bino <- glm(above_avg ~ install_log + review_log + Type + Category+ Price + Content.Rating, data=train , family="binomial")
summary(bino)
```

##Binomial backward elimination  
```{r BioRed}
bino_red <- glm(above_avg ~ install_log + review_log + Category+ Price, data=train , family="binomial")
summary(bino_red)
```  

#Google Results  
##Poisson Results  
```{r PoisRedGoog}
#exp(pois_red$coefficients)
confint.default(pois_red)

poisso_red_red <- predict(pois_red,type = 'response' )
#summary(poisso_red_red)

train$pois_red <- predict(pois_red, type = 'response')
table(actual = train$above_avg, pred = round(fitted(pois_red)))
```  

##Quasi Results  
```{r QuasRes}
#exp(quas_red$coefficients)
confint.default(quas_red)

quasi_red <- predict(quas_red,type = 'response' )
summary(quasi_red)

train$quas_red <- predict(quas_red, type = 'response')
table(actual = train$above_avg, pred = round(fitted(quas_red)))
```  

#Binomial Results  
```{r BiRes}
#exp(bino_red$coefficients)
confint.default(bino_red)

b <- predict(bino_red,type = 'response' )
#summary(b)
train$bino_red <- predict(bino_red, type = 'response')
table(actual = train$above_avg, pred = round(fitted(bino_red)))
```

For the results we took all of the reduced models and ran them on the test dataset. The Quasi results gave unexpected negavite results and the Posisson had 1 reults with a flag of 2. The Binomial reduced results were closest to the training results.  



#Conclusion  
While the Apple store data was difficult to deal with since it did not come with the number of downloads, we were still able to use a zero inflated model to get some predictions. The predictions took into a account that there were not that many apps with ratings in the millions so it had a reasonable max number of ratings, around 500,000.  

The Google Play store data was easier to work with since that did come with a total number of installs. We were abnle to get a lot more visualizations with it and run numerous models. We saw that the Binomial reduced model was the biggest help when trying to predict how many times an app woulf be downloaded.  


#Appendix  
Code for this project can be found her: https://github.com/dquarshie89/DATA-621-FINAL_PROJECT/blob/master/AppPrediction.Rmd  




